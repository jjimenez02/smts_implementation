{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerrequisitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "from sktime.datatypes._panel._convert import from_nested_to_long, from_nested_to_multi_index\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "SEED = 1\n",
    "J_INS_GRID = [20, 40, 60, 80, 100]\n",
    "N_SYMBOLS_GRID = [20, 40, 60, 80, 100]\n",
    "PKL_DIR = 'pkl/baydogan_examples/'\n",
    "\n",
    "def validate(\n",
    "        clf,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        n_times=10,\n",
    "        id_col_name='id',\n",
    "        n_jobs=-2):\n",
    "    return Parallel(n_jobs=n_jobs)(delayed(__parallel_validate)(\n",
    "        clf.clone(),\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        id_col_name=id_col_name\n",
    "    ) for _ in range(0, n_times))\n",
    "\n",
    "def __parallel_validate(\n",
    "        clf,\n",
    "        X_train,\n",
    "        X_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        id_col_name='id'):\n",
    "    return_dict = {}\n",
    "    clf.fit(X_train, y_train, id_col_name=id_col_name)\n",
    "    \n",
    "    return_dict['score'] = clf.score(\n",
    "        X_test, y_test, id_col_name=id_col_name)\n",
    "    return_dict['oob_score_'] = clf.oob_score_\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "def get_best_score(on_dicted_results):\n",
    "    best_score_mean = -np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for model in on_dicted_results.keys():\n",
    "        score_mean = on_dicted_results[model]['score_mean']\n",
    "        \n",
    "        if score_mean > best_score_mean:\n",
    "            best_score_mean = score_mean\n",
    "            best_model = model\n",
    "            \n",
    "    return best_score_mean, best_model\n",
    "\n",
    "\n",
    "def trend_fn(\n",
    "        df,\n",
    "        id_col_name='id',\n",
    "        time_col_name='time'):\n",
    "    columns = __trend_setup_cols(\n",
    "        df,\n",
    "        id_col_name=id_col_name,\n",
    "        time_col_name=time_col_name\n",
    "    )\n",
    "    data_with_features_diff = defaultdict(list)\n",
    "\n",
    "    previous_sample = None\n",
    "    for sample in df.iloc:\n",
    "        if previous_sample is None:\n",
    "            previous_sample = sample\n",
    "            continue\n",
    "\n",
    "        data_with_features_diff[columns[0]].append(sample[id_col_name])\n",
    "        data_with_features_diff[columns[1]].append(sample[time_col_name])\n",
    "\n",
    "        for i in range(2, len(columns), 2):\n",
    "            data_with_features_diff[columns[i]].append(\n",
    "                sample[columns[i]])\n",
    "            data_with_features_diff[columns[i+1]].append(\n",
    "                sample[columns[i]] - previous_sample[columns[i]])\n",
    "\n",
    "        previous_sample = sample\n",
    "\n",
    "    return pd.DataFrame(data_with_features_diff)\n",
    "\n",
    "def __trend_setup_cols(\n",
    "        df,\n",
    "        id_col_name='id',\n",
    "        time_col_name='time'):\n",
    "    final_columns = []\n",
    "\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(id_col_name)\n",
    "    columns.remove(time_col_name)\n",
    "\n",
    "    final_columns.append(id_col_name)\n",
    "    final_columns.append(time_col_name)\n",
    "    \n",
    "    for attr in columns:\n",
    "        final_columns.append(attr)\n",
    "        final_columns.append(attr + 'diff')\n",
    "\n",
    "    return final_columns\n",
    "\n",
    "def parse_into_train_test(\n",
    "        train_path,\n",
    "        test_path,\n",
    "        id_col_name='index',\n",
    "        time_col_name='time_index',\n",
    "        multivariate=False):\n",
    "    X_train, tmp_y_train =\\\n",
    "        load_from_tsfile_to_dataframe(train_path)\n",
    "    X_test, tmp_y_test =\\\n",
    "        load_from_tsfile_to_dataframe(test_path)\n",
    "        \n",
    "    if not multivariate:\n",
    "        X_train = from_nested_to_long(X_train).drop(\n",
    "            ['column'], axis=1)\n",
    "        X_test = from_nested_to_long(X_test).drop(\n",
    "            ['column'], axis=1)\n",
    "    else:\n",
    "        X_train = from_nested_to_multi_index(\n",
    "            X_train,\n",
    "            instance_index=id_col_name,\n",
    "            time_index=time_col_name\n",
    "        )\n",
    "        X_train = X_train.reset_index()\n",
    "        X_test = from_nested_to_multi_index(\n",
    "            X_test,\n",
    "            instance_index=id_col_name,\n",
    "            time_index=time_col_name\n",
    "        )\n",
    "        X_test = X_test.reset_index()\n",
    "\n",
    "    X_train, X_test = apply_trend_Fn(X_train, X_test)\n",
    "    \n",
    "    y_train = np.asarray([tmp_y_train[int(index)]\n",
    "                          for index in X_train[id_col_name]])\n",
    "    y_test = np.asarray([tmp_y_test[int(index)]\n",
    "                         for index in X_test[id_col_name]])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def apply_trend_Fn(\n",
    "        X_train,\n",
    "        X_test,\n",
    "        id_col_name='index',\n",
    "        time_col_name='time_index'):\n",
    "    new_X_train = pd.DataFrame()\n",
    "    new_X_test = pd.DataFrame()\n",
    "\n",
    "    for serie_id in pd.unique(X_train[id_col_name]):\n",
    "        serie = X_train[X_train[id_col_name] == serie_id]\n",
    "        codificated_serie = trend_fn(\n",
    "            serie, id_col_name=id_col_name, time_col_name=time_col_name)\n",
    "        new_X_train = new_X_train.append(codificated_serie)\n",
    "\n",
    "    for serie_id in pd.unique(X_test[id_col_name]):\n",
    "        serie = X_test[X_test[id_col_name] == serie_id]\n",
    "        codificated_serie = trend_fn(\n",
    "            serie, id_col_name=id_col_name, time_col_name=time_col_name)\n",
    "        new_X_test = new_X_test.append(codificated_serie)\n",
    "        \n",
    "    return new_X_train, new_X_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos\n",
    "- Este librillo intentará reproducir los resultados obtenidos en los diferentes datasets en que se probó este algoritmo (mencionados en las páginas 12-18 del paper de Mustafa Baydogan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adiac Dataset\n",
    "- Error de Baydogan: 0.248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = parse_into_train_test(\n",
    "    'data/Adiac/Adiac_TRAIN.ts',\n",
    "    'data/Adiac/Adiac_TEST.ts')\n",
    "\n",
    "X_train = X_train.drop('time_index', axis=1)\n",
    "X_test = X_test.drop('time_index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smts import SMTS\n",
    "\n",
    "all_clf_used = {}\n",
    "for j_ins in J_INS_GRID:\n",
    "    for n_symbols in N_SYMBOLS_GRID:\n",
    "        clf_results = {}\n",
    "        clf = SMTS(\n",
    "            j_ins=j_ins,\n",
    "            n_symbols=n_symbols\n",
    "        )\n",
    "\n",
    "        results = validate(\n",
    "            clf,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            n_times=10,\n",
    "            id_col_name='index',\n",
    "            n_jobs=-2\n",
    "        )\n",
    "\n",
    "        clf_scores = [result['score'] for result in results]\n",
    "        clf_results['score_mean'] = np.mean(clf_scores)\n",
    "        clf_results['score_std'] = np.std(clf_scores)\n",
    "\n",
    "        clf_oob_scores = [result['oob_score_'] for result in results]\n",
    "        clf_results['oob_score_mean'] = np.mean(clf_oob_scores)\n",
    "        clf_results['oob_score_std'] = np.std(clf_oob_scores)\n",
    "\n",
    "        all_clf_used[(j_ins, n_symbols)] = clf_results\n",
    "        print((j_ins, n_symbols))\n",
    "\n",
    "with open(PKL_DIR + 'adiac', 'wb') as file:\n",
    "    pickle.dump(all_clf_used, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PKL_DIR + 'adiac', 'rb') as file:\n",
    "    adiac_clfs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2053708439897698, (60, 80))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score_model = get_best_score(adiac_clfs)\n",
    "1-best_score_model[0], best_score_model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beef\n",
    "- Error de Baydogan: 0.26\n",
    "- La dimensión temporal aquí importa porque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = parse_into_train_test(\n",
    "    'data/Beef/Beef_TRAIN.ts',\n",
    "    'data/Beef/Beef_TEST.ts')\n",
    "\n",
    "X_train = X_train.drop('time_index', axis=1)\n",
    "X_test = X_test.drop('time_index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in pd.unique(X_train['index']):\n",
    "    if (list(X_train[X_train['index'] == 0]['time_index']) != list(X_train[X_train['index'] == x]['time_index'])):\n",
    "        print(\"LMOA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n",
      "(20, 40)\n",
      "(20, 60)\n",
      "(20, 80)\n",
      "(20, 100)\n",
      "(40, 20)\n",
      "(40, 40)\n",
      "(40, 60)\n",
      "(40, 80)\n",
      "(40, 100)\n",
      "(60, 20)\n",
      "(60, 40)\n",
      "(60, 60)\n",
      "(60, 80)\n",
      "(60, 100)\n",
      "(80, 20)\n",
      "(80, 40)\n",
      "(80, 60)\n",
      "(80, 80)\n",
      "(80, 100)\n",
      "(100, 20)\n",
      "(100, 40)\n",
      "(100, 60)\n",
      "(100, 80)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "from smts import SMTS\n",
    "\n",
    "all_clf_used = {}\n",
    "for j_ins in J_INS_GRID:\n",
    "    for n_symbols in N_SYMBOLS_GRID:\n",
    "        clf_results = {}\n",
    "        clf = SMTS(\n",
    "            j_ins=j_ins,\n",
    "            n_symbols=n_symbols\n",
    "        )\n",
    "\n",
    "        results = validate(\n",
    "            clf,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            n_times=10,\n",
    "            id_col_name='index',\n",
    "            n_jobs=-2\n",
    "        )\n",
    "\n",
    "        clf_scores = [result['score'] for result in results]\n",
    "        clf_results['score_mean'] = np.mean(clf_scores)\n",
    "        clf_results['score_std'] = np.std(clf_scores)\n",
    "\n",
    "        clf_oob_scores = [result['oob_score_'] for result in results]\n",
    "        clf_results['oob_score_mean'] = np.mean(clf_oob_scores)\n",
    "        clf_results['oob_score_std'] = np.std(clf_oob_scores)\n",
    "\n",
    "        all_clf_used[(j_ins, n_symbols)] = clf_results\n",
    "        print((j_ins, n_symbols))\n",
    "\n",
    "with open(PKL_DIR + 'beef', 'wb') as file:\n",
    "    pickle.dump(all_clf_used, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PKL_DIR + 'beef', 'rb') as file:\n",
    "    beef_clfs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_model = get_best_score(beef_clfs)\n",
    "1-best_score_model[0], best_score_model[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSU Leaf\n",
    "- Error de Baydogan: 0.377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = parse_into_train_test(\n",
    "    'data/OSULeaf/OSULeaf_TRAIN.ts',\n",
    "    'data/OSULeaf/OSULeaf_TEST.ts')\n",
    "\n",
    "X_train = X_train.drop('time_index', axis=1)\n",
    "X_test = X_test.drop('time_index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n",
      "(20, 40)\n",
      "(20, 60)\n",
      "(20, 80)\n",
      "(20, 100)\n",
      "(40, 20)\n",
      "(40, 40)\n",
      "(40, 60)\n",
      "(40, 80)\n",
      "(40, 100)\n",
      "(60, 20)\n",
      "(60, 40)\n",
      "(60, 60)\n",
      "(60, 80)\n",
      "(60, 100)\n",
      "(80, 20)\n",
      "(80, 40)\n",
      "(80, 60)\n",
      "(80, 80)\n",
      "(80, 100)\n",
      "(100, 20)\n",
      "(100, 40)\n",
      "(100, 60)\n",
      "(100, 80)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "from smts import SMTS\n",
    "\n",
    "all_clf_used = {}\n",
    "for j_ins in J_INS_GRID:\n",
    "    for n_symbols in N_SYMBOLS_GRID:\n",
    "        clf_results = {}\n",
    "        clf = SMTS(\n",
    "            j_ins=j_ins,\n",
    "            n_symbols=n_symbols\n",
    "        )\n",
    "\n",
    "        results = validate(\n",
    "            clf,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            n_times=10,\n",
    "            id_col_name='index',\n",
    "            n_jobs=-2\n",
    "        )\n",
    "\n",
    "        clf_scores = [result['score'] for result in results]\n",
    "        clf_results['score_mean'] = np.mean(clf_scores)\n",
    "        clf_results['score_std'] = np.std(clf_scores)\n",
    "\n",
    "        clf_oob_scores = [result['oob_score_'] for result in results]\n",
    "        clf_results['oob_score_mean'] = np.mean(clf_oob_scores)\n",
    "        clf_results['oob_score_std'] = np.std(clf_oob_scores)\n",
    "\n",
    "        all_clf_used[(j_ins, n_symbols)] = clf_results\n",
    "        print((j_ins, n_symbols))\n",
    "\n",
    "with open(PKL_DIR + 'osuleaf', 'wb') as file:\n",
    "    pickle.dump(all_clf_used, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PKL_DIR + 'osuleaf', 'rb') as file:\n",
    "    osuleaf_clfs = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.32272727272727264, (80, 40))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score_model = get_best_score(osuleaf_clfs)\n",
    "1-best_score_model[0], best_score_model[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBF\n",
    "- Error de Baydogan: 0.020\n",
    "- De nuevo ha aumentado mucho el error -> Aquí importa el índice temporal porque añade un offset a las series (puede influir entonces el momento en que ocurre uno u otro evento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = parse_into_train_test(\n",
    "    'data/CBF/CBF_TRAIN.ts',\n",
    "    'data/CBF/CBF_TEST.ts')\n",
    "\n",
    "# X_train = X_train.drop('time_index', axis=1)\n",
    "# X_test = X_test.drop('time_index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in pd.unique(X_train['index']):\n",
    "    if (list(X_train[X_train['index'] == 0]['time_index']) != list(X_train[X_train['index'] == x]['time_index'])):\n",
    "        print(\"LMOA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smts import SMTS\n",
    "\n",
    "all_clf_used = {}\n",
    "for j_ins in J_INS_GRID:\n",
    "    for n_symbols in N_SYMBOLS_GRID:\n",
    "        clf_results = {}\n",
    "        clf = SMTS(\n",
    "            j_ins=j_ins,\n",
    "            n_symbols=n_symbols\n",
    "        )\n",
    "\n",
    "        results = validate(\n",
    "            clf,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            n_times=10,\n",
    "            id_col_name='index',\n",
    "            n_jobs=-2\n",
    "        )\n",
    "\n",
    "        clf_scores = [result['score'] for result in results]\n",
    "        clf_results['score_mean'] = np.mean(clf_scores)\n",
    "        clf_results['score_std'] = np.std(clf_scores)\n",
    "\n",
    "        clf_oob_scores = [result['oob_score_'] for result in results]\n",
    "        clf_results['oob_score_mean'] = np.mean(clf_oob_scores)\n",
    "        clf_results['oob_score_std'] = np.std(clf_oob_scores)\n",
    "\n",
    "        all_clf_used[(j_ins, n_symbols)] = clf_results\n",
    "        print((j_ins, n_symbols))\n",
    "\n",
    "with open(PKL_DIR + 'cbf', 'wb') as file:\n",
    "    pickle.dump(all_clf_used, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PKL_DIR + 'cbf', 'rb') as file:\n",
    "    cbf_clfs = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_model = get_best_score(cbf_clfs)\n",
    "1-best_score_model[0], best_score_model[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Japanese Vowels\n",
    "- Error de Baydogan: 0.031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = parse_into_train_test(\n",
    "    'data/JapaneseVowels/JapaneseVowels_TRAIN.ts',\n",
    "    'data/JapaneseVowels/JapaneseVowels_TEST.ts',\n",
    "    multivariate=True)\n",
    "\n",
    "X_train = X_train.drop('time_index', axis=1)\n",
    "X_test = X_test.drop('time_index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n",
      "(20, 40)\n",
      "(20, 60)\n",
      "(20, 80)\n",
      "(20, 100)\n",
      "(40, 20)\n",
      "(40, 40)\n",
      "(40, 60)\n",
      "(40, 80)\n",
      "(40, 100)\n",
      "(60, 20)\n",
      "(60, 40)\n",
      "(60, 60)\n",
      "(60, 80)\n",
      "(60, 100)\n",
      "(80, 20)\n",
      "(80, 40)\n",
      "(80, 60)\n",
      "(80, 80)\n",
      "(80, 100)\n",
      "(100, 20)\n",
      "(100, 40)\n",
      "(100, 60)\n",
      "(100, 80)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "from smts import SMTS\n",
    "\n",
    "all_clf_used = {}\n",
    "for j_ins in J_INS_GRID:\n",
    "    for n_symbols in N_SYMBOLS_GRID:\n",
    "        clf_results = {}\n",
    "        clf = SMTS(\n",
    "            j_ins=j_ins,\n",
    "            n_symbols=n_symbols\n",
    "        )\n",
    "\n",
    "        results = validate(\n",
    "            clf,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            n_times=10,\n",
    "            id_col_name='index',\n",
    "            n_jobs=-2\n",
    "        )\n",
    "\n",
    "        clf_scores = [result['score'] for result in results]\n",
    "        clf_results['score_mean'] = np.mean(clf_scores)\n",
    "        clf_results['score_std'] = np.std(clf_scores)\n",
    "\n",
    "        clf_oob_scores = [result['oob_score_'] for result in results]\n",
    "        clf_results['oob_score_mean'] = np.mean(clf_oob_scores)\n",
    "        clf_results['oob_score_std'] = np.std(clf_oob_scores)\n",
    "\n",
    "        all_clf_used[(j_ins, n_symbols)] = clf_results\n",
    "        print((j_ins, n_symbols))\n",
    "\n",
    "with open(PKL_DIR + 'japanese_vowels', 'wb') as file:\n",
    "    pickle.dump(all_clf_used, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PKL_DIR + 'japanese_vowels', 'rb') as file:\n",
    "    japanese_vowels_clfs = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.029189189189189113, (20, 80))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score_model = get_best_score(japanese_vowels_clfs)\n",
    "1-best_score_model[0], best_score_model[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libras\n",
    "- Error de Baydogan: 0.091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = parse_into_train_test(\n",
    "    'data/Libras/Libras_TRAIN.ts',\n",
    "    'data/Libras/Libras_TEST.ts',\n",
    "    multivariate=True)\n",
    "\n",
    "X_train = X_train.drop('time_index', axis=1)\n",
    "X_test = X_test.drop('time_index', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20)\n",
      "(20, 40)\n",
      "(20, 60)\n",
      "(20, 80)\n",
      "(20, 100)\n",
      "(40, 20)\n",
      "(40, 40)\n",
      "(40, 60)\n",
      "(40, 80)\n",
      "(40, 100)\n",
      "(60, 20)\n",
      "(60, 40)\n",
      "(60, 60)\n",
      "(60, 80)\n",
      "(60, 100)\n",
      "(80, 20)\n",
      "(80, 40)\n",
      "(80, 60)\n",
      "(80, 80)\n",
      "(80, 100)\n",
      "(100, 20)\n",
      "(100, 40)\n",
      "(100, 60)\n",
      "(100, 80)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "from smts import SMTS\n",
    "\n",
    "all_clf_used = {}\n",
    "for j_ins in J_INS_GRID:\n",
    "    for n_symbols in N_SYMBOLS_GRID:\n",
    "        clf_results = {}\n",
    "        clf = SMTS(\n",
    "            j_ins=j_ins,\n",
    "            n_symbols=n_symbols\n",
    "        )\n",
    "\n",
    "        results = validate(\n",
    "            clf,\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test,\n",
    "            n_times=10,\n",
    "            id_col_name='index',\n",
    "            n_jobs=-2\n",
    "        )\n",
    "\n",
    "        clf_scores = [result['score'] for result in results]\n",
    "        clf_results['score_mean'] = np.mean(clf_scores)\n",
    "        clf_results['score_std'] = np.std(clf_scores)\n",
    "\n",
    "        clf_oob_scores = [result['oob_score_'] for result in results]\n",
    "        clf_results['oob_score_mean'] = np.mean(clf_oob_scores)\n",
    "        clf_results['oob_score_std'] = np.std(clf_oob_scores)\n",
    "\n",
    "        all_clf_used[(j_ins, n_symbols)] = clf_results\n",
    "        print((j_ins, n_symbols))\n",
    "\n",
    "with open(PKL_DIR + 'libras', 'wb') as file:\n",
    "    pickle.dump(all_clf_used, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PKL_DIR + 'libras', 'rb') as file:\n",
    "    libras_clfs = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10055555555555562, (100, 40))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score_model = get_best_score(libras_clfs)\n",
    "1-best_score_model[0], best_score_model[1]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efb1675046944d5baa2fc0fdb51665fa47716b7e8f420db1fbed19be946b66c7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('tfg_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
